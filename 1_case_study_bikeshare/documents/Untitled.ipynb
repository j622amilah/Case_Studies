{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b49b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.cloud import bigquery\n",
    "\n",
    "SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, \n",
    "ORDINAL_POSITION as org_pos, DATA_TYPE, CHARACTER_MAXIMUM_LENGTH as CML\n",
    "FROM INFORMATION_SCHEMA.COLUMNS\n",
    "WHERE COLUMN_NAME like '%Err%'\n",
    "ORDER BY TABLE_NAME\n",
    "\n",
    "\n",
    "SELECT col.name AS [Column Name], tab.name AS [Table Name]\n",
    "FROM sys.columns col\n",
    "INNER JOIN sys.tables tab ON col.object_id = tab.object_id\n",
    "WHERE col.name LIKE '%Name%'\n",
    "ORDER BY [Table Name], [Column Name]\n",
    "\n",
    "def BigQuery_SQL():\n",
    "\t\n",
    "\t# Structured Query Language (SQL) and BigQuery\n",
    "    \n",
    "    # The following steps are executed :\n",
    "    # Client object -> project -> dataset -> table -> set limitations on queries, query the table\n",
    "    \n",
    "\n",
    "    # ----------------\n",
    "\t# bigquery.Client()\n",
    "    # ----------------\n",
    "    # Create a \"Client\" object\n",
    "\tclient = bigquery.Client()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # client.dataset()\n",
    "    # ----------------\n",
    "    # Get the project\n",
    "    \n",
    "    # project name :  bigquery-public-data\n",
    "    # dataset name : hacker_news\n",
    "    \n",
    "    # Construct a reference to the dataset\n",
    "    dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------\n",
    "    # client.get_dataset()\n",
    "    # ----------------\n",
    "    # Get the dataset\n",
    "    # API request - fetch the dataset\n",
    "    dataset = client.get_dataset(dataset_ref)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # client.list_tables()\n",
    "    # ----------------\n",
    "    # List all the tables in the dataset\n",
    "    tables = list(client.list_tables(dataset))\n",
    "\n",
    "    # Print names of all tables in the dataset (there are four!)\n",
    "    for table in tables:  \n",
    "        print(table.table_id)\n",
    "    # Results:\n",
    "    # comments\n",
    "    # full\n",
    "    # full_201510\n",
    "    # stories\n",
    "\n",
    "    # OR\n",
    "    out = [table.table_id for table in tables]\n",
    "    print(out)\n",
    "    \n",
    "\n",
    "\n",
    "    # ----------------\n",
    "    # client.get_table()\n",
    "    # ----------------\n",
    "    # Fetching table (full) in the dataset (hacker_news)\n",
    "    \n",
    "    # Construct a reference to the table\n",
    "    table_ref = dataset_ref.table(\"full\")    # table name = full\n",
    "\n",
    "    # API request - fetch the table\n",
    "    table = client.get_table(table_ref)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # SchemaField\n",
    "    # ----------------\n",
    "    # The structure of a table is called its schema.\n",
    "    # Each SchemaField tells us about a specific column/field in order : \n",
    "    # 1) name of the column, \n",
    "    # 2) field type (or datatype) in the column, \n",
    "    # 3) mode of the column ('NULLABLE' means that a column allows NULL values, and is the default), \n",
    "    # 4) description of the data in that column\n",
    "    \n",
    "    \n",
    "    # Print information on all the columns in the \"full\" table in the \"hacker_news\" dataset\n",
    "    table.schema\n",
    "    # OR\n",
    "    print(table.schema)\n",
    "    \n",
    "    # Result:\n",
    "    # [SchemaField('title', 'STRING', 'NULLABLE', 'Story title', (), None),\n",
    "     # SchemaField('url', 'STRING', 'NULLABLE', 'Story url', (), None),\n",
    "     # SchemaField('text', 'STRING', 'NULLABLE', 'Story or comment text', (), None),\n",
    "     # SchemaField('dead', 'BOOLEAN', 'NULLABLE', 'Is dead?', (), None),\n",
    "     # SchemaField('by', 'STRING', 'NULLABLE', \"The username of the item's author.\", (), None),\n",
    "     # SchemaField('score', 'INTEGER', 'NULLABLE', 'Story score', (), None),\n",
    "     # SchemaField('time', 'INTEGER', 'NULLABLE', 'Unix time', (), None),\n",
    "     # SchemaField('timestamp', 'TIMESTAMP', 'NULLABLE', 'Timestamp for the unix time', (), None),\n",
    "     # SchemaField('type', 'STRING', 'NULLABLE', 'Type of details (comment, comment_ranking, poll, story, job, pollopt)', (), None),\n",
    "     # SchemaField('id', 'INTEGER', 'NULLABLE', \"The item's unique id.\", (), None),\n",
    "     # SchemaField('parent', 'INTEGER', 'NULLABLE', 'Parent comment ID', (), None),\n",
    "     # SchemaField('descendants', 'INTEGER', 'NULLABLE', 'Number of story or poll descendants', (), None),\n",
    "     # SchemaField('ranking', 'INTEGER', 'NULLABLE', 'Comment ranking', (), None),\n",
    "     # SchemaField('deleted', 'BOOLEAN', 'NULLABLE', 'Is deleted?', (), None)]\n",
    "\n",
    "    num_of_columns = len(table.schema)\n",
    "    print('num_of_columns : ', num_of_columns)\n",
    "    \n",
    "    # ----------------\n",
    "    # View a certain number of rows in a pandas dataframe\n",
    "    # ----------------\n",
    "    # list_rows() : view first 5 lines of the table \n",
    "    # to_dataframe() : convert lines to a dataFrame\n",
    "    \n",
    "    # Combine list_rows() and to_dataframe() to view the first five lines in a dataFrame format\n",
    "    client.list_rows(table, max_results=5).to_dataframe()\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Preview the first five entries in the \"by\" column of the \"full\" table\n",
    "    client.list_rows(table, selected_fields=table.schema[:1], max_results=5).to_dataframe()\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # How to index a SchemaField\n",
    "    # https://googleapis.dev/python/bigquery/latest/generated/google.cloud.bigquery.schema.SchemaField.html\n",
    "    \n",
    "    # SchemaField('date', 'TIMESTAMP', 'NULLABLE', 'Date when the incident occurred. this is sometimes a best estimate.', (), None)\n",
    "    \n",
    "    table.schema[col_num].name        # The name of the field.\n",
    "    table.schema[col_num].field_type    # The type of the field\n",
    "    table.schema[col_num].mode      # Defaults to 'NULLABLE'. The mode of the field. \n",
    "    table.schema[col_num].description         # Description for the field\n",
    "    table.schema[col_num].fields   # Subfields (requires field_type of ‘RECORD’)\n",
    "    table.schema[col_num].policy_tags    # The policy tag list for the field.\n",
    "    table.schema[col_num].precision   # Precison (number of digits) of fields with NUMERIC or BIGNUMERIC type.\n",
    "    table.schema[col_num].scale   # Scale (digits after decimal) of fields with NUMERIC or BIGNUMERIC type.\n",
    "    table.schema[col_num].max_length   # Maximum length of fields with STRING or BYTES type\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # How many columns in the crime table have TIMESTAMP data?\n",
    "    r = []     \n",
    "    for i in range(len(table.schema)):\n",
    "        r = r + [table.schema[i].field_type  == 'TIMESTAMP']\n",
    "    print('r : ', r)\n",
    "\n",
    "    num_timestamp = len([i for i in range(len(r)) if r[i] == True])\n",
    "    print('num_timestamp : ', num_timestamp)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Examples of the above procedure:\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Create a \"Client\" object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Construct a reference to the \"san_francisco\" dataset\n",
    "    dataset_ref = client.dataset(\"san_francisco\", project=\"bigquery-public-data\")\n",
    "\n",
    "    # API request - fetch the dataset\n",
    "    dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "    # Construct a reference to the \"bikeshare_trips\" table\n",
    "    table_ref = dataset_ref.table(\"bikeshare_trips\")\n",
    "\n",
    "    # API request - fetch the table\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # Preview the first five lines of the table\n",
    "    client.list_rows(table, max_results=5).to_dataframe()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Create a \"Client\" object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Construct a reference to the \"stackoverflow\" dataset\n",
    "    dataset_ref = client.dataset(\"stackoverflow\", project=\"bigquery-public-data\")\n",
    "\n",
    "    # API request - fetch the dataset\n",
    "    dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "    # List the names of the tables in the dataset\n",
    "    tables = list(client.list_tables(dataset))\n",
    "    for table in tables:\n",
    "        print(table.table_id)\n",
    "\n",
    "    # Now open up the posts_questions table\n",
    "    # Construct a reference to the \"posts_questions\" table\n",
    "    table_ref = dataset_ref.table(\"posts_questions\")\n",
    "\n",
    "    # API request - fetch the table\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # Preview the first five lines of the table\n",
    "    client.list_rows(table, max_results=5).to_dataframe()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Get the table of the QUESTIONS in a dataframe\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT id, title, accepted_answer_id, creation_date, owner_user_id, parent_id\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` \n",
    "        WHERE creation_date >= '2018-01-01' and creation_date < '2018-02-01'\n",
    "        ORDER BY id DESC\n",
    "        \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "    df_questions = query_job.to_dataframe()\n",
    "    df_questions.head()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Get the table of the ANSWERS in a dataframe\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT id, title, accepted_answer_id, creation_date, owner_user_id, parent_id\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_answers` \n",
    "        WHERE creation_date >= '2018-01-01' and creation_date < '2018-02-01'\n",
    "        ORDER BY parent_id DESC\n",
    "        \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "    df_answers = query_job.to_dataframe()\n",
    "    df_answers.head()\n",
    "        \n",
    "    # ----------------\n",
    "    # client.query()\n",
    "    # ----------------\n",
    "    # The keywords in the query string, SELECT, FROM WHERE, are not case sensitive\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Select a single column from a single table\n",
    "    SELECT name_of_column\n",
    "    FROM `project_name.dataset_name.table_name`\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # WHERE\n",
    "    # ----------------\n",
    "    # To select a column that contains an entry (text)\n",
    "    WHERE name_of_column = 'text_entry'\n",
    "    # OR\n",
    "    WHERE name_of_column = value_entry\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # WHERE ... LIKE, WHERE ... CONTAINS\n",
    "    # ----------------\n",
    "    # Selects text or a value that is EQUAL to the value after LIKE, from the name_of_column\n",
    "    \n",
    "    WHERE name_of_column LIKE 'text_entry'\n",
    "    # OR\n",
    "    WHERE name_of_column LIKE value_entry\n",
    "    # OR\n",
    "    WHERE name_of_column = value_entry\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Special characters (%) : \n",
    "    # The characters between the percent sign are found in the given order, regardless of case (upper, lower). It will also select words with hyphens before and after the percent sign. \n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM `bigquery-public-data.pet_records.pets` \n",
    "        WHERE Name LIKE '%ipl%'\n",
    "        \"\"\"\n",
    "    \n",
    "    # it will find characters such as : 'Ipl', 'g-ipl-2', '-IPL' \n",
    "    \n",
    "    # OR\n",
    "    \n",
    "    query = \"\"\"\n",
    "        SELECT * \n",
    "        FROM `bigquery-public-data.pet_records.pets` \n",
    "        WHERE Name CONTAINS 'ipl'\n",
    "        \"\"\"\n",
    "    \n",
    "    # (WHERE ... LIKE % %) is the same as (WHERE ... CONTAINS), people perfer CONTAINS because it is more human friendly/readable.\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # The 3 double quotes in the query denote the start and end of the string command\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Query to select all the items from the \"city\" column where the \"country\" column is 'US'\n",
    "    query = \"\"\"\n",
    "            SELECT city\n",
    "            FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "            WHERE country = 'US'\n",
    "            \"\"\"\n",
    "    # ----------------\n",
    "    \n",
    "    # Submitting the query to the dataset\n",
    "    \n",
    "    # 1) Create a \"Client\" object\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # 2)  Set up the query\n",
    "    query_job = client.query(query)\n",
    "    \n",
    "    # Run the query and return a pandas DataFrame\n",
    "    us_cities = query_job.to_dataframe()\n",
    "    \n",
    "    # See the top five unique counted values in the city column\n",
    "    us_cities.city.value_counts().head()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # To select items from 2 columns \n",
    "    query = \"\"\" \n",
    "        SELECT city, country\n",
    "        FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "        WHERE country = 'US'\n",
    "        \"\"\"\n",
    "        \n",
    "    # ----------------\n",
    "    \n",
    "    # To select items in all columns where country equals 'US'\n",
    "    query = \"\"\"\n",
    "            SELECT *\n",
    "            FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "            WHERE country = 'US'\n",
    "            \"\"\"\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Determine how much data the query will scan\n",
    "    \n",
    "    # Query to get the score column from every row where the type column has value \"job\"\n",
    "    query = \"\"\"\n",
    "            SELECT score, title\n",
    "            FROM `bigquery-public-data.hacker_news.full`\n",
    "            WHERE type = \"job\" \n",
    "            \"\"\"\n",
    "\n",
    "    # Create a QueryJobConfig object to estimate size of query without running it\n",
    "    dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "\n",
    "    # API request - dry run query to estimate costs\n",
    "    dry_run_query_job = client.query(query, job_config=dry_run_config)\n",
    "\n",
    "    print(\"This query will process {} bytes.\".format(dry_run_query_job.total_bytes_processed))\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Limit the query : only allow the query to run if it uses less than a certain amount of data \n",
    "    \n",
    "    \n",
    "    ONE_MB = 1000*1000          # Only run the query if it's less than 1 MB\n",
    "    \n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_MB)\n",
    "\n",
    "    # Set up the query (will only run if it's less than 1 MB)\n",
    "    safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "    # API request - try to run the query, and return a pandas DataFrame\n",
    "    safe_query_job.to_dataframe()\n",
    "    \n",
    "    # It returned 2 rows and stopped, the query was cancelled because the limit of 1 MB was exceeded\n",
    "    # Try it again with more data quota\n",
    "    \n",
    "    ONE_GB = 1000*1000*1000     # Only run the query if it's less than 1 GB\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=ONE_GB)\n",
    "\n",
    "    # Set up the query (will only run if it's less than 1 GB)\n",
    "    safe_query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "    # API request - try to run the query, and return a pandas DataFrame\n",
    "    job_post_scores = safe_query_job.to_dataframe()\n",
    "\n",
    "    # Print average score for job posts\n",
    "    job_post_scores.score.mean()\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Example: \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Create a \"Client\" object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Construct a reference to the \"openaq\" dataset\n",
    "    dataset_ref = client.dataset(\"openaq\", project=\"bigquery-public-data\")\n",
    "\n",
    "    # API request - fetch the dataset\n",
    "    dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "    # Construct a reference to the \"global_air_quality\" table\n",
    "    table_ref = dataset_ref.table(\"global_air_quality\")\n",
    "\n",
    "    # API request - fetch the table\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # Preview the first five lines of the \"global_air_quality\" table\n",
    "    client.list_rows(table, max_results=5).to_dataframe()\n",
    "    \n",
    "    # Query to select countries with units of \"ppm\"\n",
    "    # parts per million can be expressed as milligrams per liter (mg/L).\n",
    "    first_query = \"\"\"\n",
    "                SELECT country\n",
    "                FROM `bigquery-public-data.openaq.global_air_quality`\n",
    "                WHERE unit = 'ppm'\n",
    "                \"\"\"\n",
    "    # Your code goes here\n",
    "\n",
    "    # Set up the query (cancel the query if it would use too much of \n",
    "    # your quota, with the limit set to 10 GB)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    first_query_job = client.query(first_query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    first_results = first_query_job.to_dataframe()\n",
    "\n",
    "    # View top few rows of results\n",
    "    print(first_results.head())\n",
    "    \n",
    "    # ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377d3b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    # ----------------\n",
    "    # COUNT()\n",
    "    # ----------------\n",
    "    # Returns a count the for an expression/operation for a column \n",
    "    \n",
    "    # Returns the count of the column ID\n",
    "    first_query = \"\"\"\n",
    "                SELECT COUNT(ID)\n",
    "                FROM `bigquery-public-data.pet_records.pets`\n",
    "                \"\"\"\n",
    "    \n",
    "    # It creates a column named f0_ with the count result\n",
    "    \n",
    "    # ----------------\n",
    "    # SUM(), AVG(), MIN(), MAX()\n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # GROUP BY()\n",
    "    # ----------------\n",
    "    # For one are more columns, it finds the unique values across rows\n",
    "    \n",
    "    # The selected rows and columns are considered a group thx\n",
    "    \n",
    "    # Returns the count of unique animals found in column Animal.\n",
    "    first_query = \"\"\"\n",
    "                SELECT Animal, COUNT(ID)\n",
    "                FROM `bigquery-public-data.pet_records.pets`\n",
    "                GROUP BY Animal\n",
    "                \"\"\"\n",
    "    # It creates a column named f0_ where it repeats the count for each animal in the Animal column\n",
    "    \n",
    "    # GROUP BY and aggregate functions (COUNT, MIN, MAX, etc) are used together. \n",
    "    # The columns that you pass to the aggregate function needs to be passed to GROUP BY.\n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # GROUP BY()\n",
    "    # ----------------\n",
    "    # For one are more columns, it finds the unique values across rows.  And applies a logic statement tp the result, to eliminate some result values\n",
    "    \n",
    "    # Returns the count of unique animals found in column Animal, that have a count > 1\n",
    "    \n",
    "    first_query = \"\"\"\n",
    "                SELECT Animal, COUNT(ID)\n",
    "                FROM `bigquery-public-data.pet_records.pets`\n",
    "                GROUP BY Animal\n",
    "                HAVING COUNT(ID) > 1\n",
    "                \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    # parent column indicates the comment \n",
    "    # id column has the unique ID used to identify each comment\n",
    "    \n",
    "    # Returns the unique comments that are counted more than 10 times\n",
    "    query_popular = \"\"\"\n",
    "                    SELECT parent, COUNT(id)\n",
    "                    FROM `bigquery-public-data.hacker_news.comments`\n",
    "                    GROUP BY parent\n",
    "                    HAVING COUNT(id) > 10\n",
    "                    \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # AS\n",
    "    # ----------------\n",
    "    # Aliasing - change the name of a returned column from the COUNT command\n",
    "    \n",
    "    # Returns the unique comments that are counted more than 10 times.  the returned column f0_ is renamed as NumPosts.\n",
    "    query_improved = \"\"\"\n",
    "                     SELECT parent, COUNT(1) AS NumPosts\n",
    "                     FROM `bigquery-public-data.hacker_news.comments`\n",
    "                     GROUP BY parent\n",
    "                     HAVING COUNT(1) > 10\n",
    "                     \"\"\"\n",
    "\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    query_job = client.query(query_improved, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and convert the results to a pandas DataFrame\n",
    "    improved_df = query_job.to_dataframe()\n",
    "\n",
    "    # Print the first five rows of the DataFrame\n",
    "    improved_df.head()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Create a \"Client\" object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Construct a reference to the \"hacker_news\" dataset\n",
    "    dataset_ref = client.dataset(\"hacker_news\", project=\"bigquery-public-data\")\n",
    "\n",
    "    # API request - fetch the dataset\n",
    "    dataset = client.get_dataset(dataset_ref)\n",
    "\n",
    "    # Construct a reference to the \"comments\" table\n",
    "    table_ref = dataset_ref.table(\"comments\")\n",
    "\n",
    "    # API request - fetch the table\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # Preview the first five lines of the \"comments\" table\n",
    "    client.list_rows(table, max_results=5).to_dataframe()\n",
    "    \n",
    "    # Query to select prolific commenters and post counts\n",
    "    prolific_commenters_query = \"\"\"\n",
    "            SELECT author, COUNT(1) AS NumPosts\n",
    "            FROM `bigquery-public-data.hacker_news.comments`\n",
    "            GROUP BY author\n",
    "            HAVING COUNT(1) > 10000\n",
    "            \"\"\"\n",
    "    # COUNT(1) is the index column\n",
    "\n",
    "    # Set up the query (cancel the query if it would use too much of \n",
    "    # your quota, with the limit set to 1 GB)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    query_job = client.query(prolific_commenters_query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    prolific_commenters = query_job.to_dataframe()\n",
    "\n",
    "    # View top few rows of results\n",
    "    print(prolific_commenters.head())\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # ----------------\n",
    "    # ORDER BY\n",
    "    # ----------------\n",
    "    # Sort the order of the result in ascending order, is usually the last clause in your query\n",
    "    \n",
    "    # To sort the result by the ID column\n",
    "    query = \"\"\"\n",
    "            SELECT ID, Name, Animal\n",
    "            FROM `bigquery-public-data.pet_records.pets`\n",
    "            ORDER BY ID\n",
    "            \"\"\"\n",
    "    \n",
    "    # Columns that have text are orded in alphabetical order\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # ORDER BY ... DESC\n",
    "    # ----------------\n",
    "    Sort the order of the result in descending order\n",
    "    # To sort the result by the Animal column\n",
    "    query = \"\"\"\n",
    "            SELECT ID, Name, Animal\n",
    "            FROM `bigquery-public-data.pet_records.pets`\n",
    "            ORDER BY Animal DESC\n",
    "            \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # DATE\n",
    "    # ----------------\n",
    "    # DATE format : YYYY-[M]M-[D]D\n",
    "    \n",
    "    # DATETIME format is like the date format,  but with time added at the end.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # EXTRACT\n",
    "    # ----------------\n",
    "    Select information (ie : Day, Week, etc) from a column (ie : DATE or DATETIME)\n",
    "    \n",
    "    # Take the Day information from Date format\n",
    "    query = \"\"\"\n",
    "            SELECT ID, Name, EXTRACT(DAY from Date) AS Day\n",
    "            FROM `bigquery-public-data.pet_records.pets_with_date`\n",
    "            \"\"\"\n",
    "            \n",
    "    # ----------------\n",
    "            \n",
    "    # Calculate information from Date format (ie : Week)\n",
    "    query = \"\"\"\n",
    "            SELECT ID, Name, EXTRACT(WEEK from Date) AS Week\n",
    "            FROM `bigquery-public-data.pet_records.pets_with_date`\n",
    "            \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Which day of the week has the most fatal motor accidents?\n",
    "    # consecutive_number column contains a unique ID for each accident\n",
    "    # timestamp_of_crash column contains the date of the accident in DATETIME format\n",
    "    query = \"\"\"\n",
    "        SELECT COUNT(consecutive_number) AS num_accidents, \n",
    "               EXTRACT(DAYOFWEEK FROM timestamp_of_crash) AS day_of_week\n",
    "        FROM `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015`\n",
    "        GROUP BY day_of_week\n",
    "        ORDER BY num_accidents DESC\n",
    "        \"\"\"\n",
    "        \n",
    "    # ----------------\n",
    "    \n",
    "    SELECT EXTRACT(YEAR FROM STARTTIME) AS year, COUNT(*) AS number_of_rides\n",
    "    FROM bigquery-public-data.new_york.citibike_trips`\n",
    "    GROUP BY year\n",
    "    ORDER BY year\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Which countries spend the largest fraction of GDP on education? \n",
    "    \n",
    "    # want rows where indicator_code == 'SE.XPD.TOTL.GD.ZS'\n",
    "    # of these rows\n",
    "    # get the unique countries and take the avg of their value column from years 2010-2017 (including 2010 and 2017 in the average)\n",
    "    \n",
    "    # Your code goes here\n",
    "    country_spend_pct_query = \"\"\"\n",
    "                              SELECT country_name, AVG(value) AS avg_ed_spending_pct\n",
    "                              FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
    "                              WHERE indicator_code = 'SE.XPD.TOTL.GD.ZS' and year >= 2010 and year <= 2017\n",
    "                              GROUP BY country_name\n",
    "                              ORDER BY avg_ed_spending_pct DESC\n",
    "                              \"\"\"\n",
    "\n",
    "    # Set up the query (cancel the query if it would use too much of \n",
    "    # your quota, with the limit set to 1 GB)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    country_spend_pct_query_job = client.query(country_spend_pct_query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    country_spending_results = country_spend_pct_query_job.to_dataframe()\n",
    "\n",
    "    # View top few rows of results\n",
    "    print(country_spending_results.head())\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Selects the indicator code and indicator name for all codes with at least 175 rows in the year 2016.\n",
    "    Order from results most frequent to least frequent.\n",
    "    \n",
    "    # Your code goes here\n",
    "    code_count_query = \"\"\"\n",
    "                    SELECT indicator_code, indicator_name, COUNT(indicator_code) AS num_rows\n",
    "                    FROM `bigquery-public-data.world_bank_intl_education.international_education`\n",
    "                    WHERE year = 2016\n",
    "                    GROUP BY indicator_code, indicator_name\n",
    "                    HAVING COUNT(indicator_code) >= 175\n",
    "                    ORDER BY COUNT(indicator_code) DESC\n",
    "                        \"\"\"\n",
    "\n",
    "    # Set up the query\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    code_count_query_job = client.query(code_count_query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    code_count_results = code_count_query_job.to_dataframe()\n",
    "\n",
    "    # View top few rows of results\n",
    "    print(code_count_results.head())\n",
    "    \n",
    "    # Result:\n",
    "              # indicator_code                   indicator_name  num_rows\n",
    "    # 0        SP.POP.GROW     Population growth (annual %)       232\n",
    "    # 1        SP.POP.TOTL                Population, total       232\n",
    "    # 2     IT.NET.USER.P2  Internet users (per 100 people)       223\n",
    "    # 3  SP.POP.1564.FE.IN   Population, ages 15-64, female       213\n",
    "    # 4  SP.POP.TOTL.MA.IN                 Population, male       213\n",
    "\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # WITH ... AS\n",
    "    # ----------------\n",
    "    # A common table expression (or CTE) is a temporary table that you return within your query. CTEs are helpful for splitting your queries into readable chunks, and you can write queries against them\n",
    "    \n",
    "    # Save the result of a short query, such that you can use it again \n",
    "    query = \"\"\"\n",
    "            WITH Seniors AS\n",
    "            (\n",
    "                SELECT ID, Name\n",
    "                FROM `bigquery-public-data.pet_records.pets`\n",
    "                WHERE Years_old > 5\n",
    "            )\n",
    "            SELECT ID\n",
    "            FROM Seniors\n",
    "                \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "\n",
    "    # Query to select the number of transactions per date, sorted by date\n",
    "    query_with_CTE = \"\"\" \n",
    "                     WITH time AS \n",
    "                     (\n",
    "                         SELECT DATE(block_timestamp) AS trans_date\n",
    "                         FROM `bigquery-public-data.crypto_bitcoin.transactions`\n",
    "                     )\n",
    "                     SELECT COUNT(1) AS transactions, trans_date\n",
    "                     FROM time\n",
    "                     GROUP BY trans_date\n",
    "                     ORDER BY trans_date\n",
    "                     \"\"\"\n",
    "\n",
    "    # Set up the query (cancel the query if it would use too much of \n",
    "    # your quota, with the limit set to 10 GB)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    query_job = client.query(query_with_CTE, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and convert the results to a pandas DataFrame\n",
    "    transactions_by_date = query_job.to_dataframe()\n",
    "\n",
    "    # Print the first five rows\n",
    "    transactions_by_date.head()\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    rides_per_year_query = \"\"\"\n",
    "                    WITH name1 AS \n",
    "                     (\n",
    "                         SELECT EXTRACT(YEAR FROM trip_start_timestamp) AS year\n",
    "                         FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                     )\n",
    "                     SELECT year, COUNT(1) AS num_trips\n",
    "                     FROM name1\n",
    "                     GROUP BY year\n",
    "                     ORDER BY year\n",
    "                        \"\"\"\n",
    "                        \n",
    "    # OR\n",
    "    \n",
    "    rides_per_year_query = \"\"\"\n",
    "                    SELECT EXTRACT(YEAR FROM trip_start_timestamp) AS year, COUNT(unique_key) AS num_trips\n",
    "                    FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                     GROUP BY year\n",
    "                     ORDER BY year\n",
    "                        \"\"\"\n",
    "\n",
    "    # Set up the query (cancel the query if it would use too much of \n",
    "    # your quota)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    rides_per_year_query_job = client.query(rides_per_year_query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    rides_per_year_result = rides_per_year_query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "    # View results\n",
    "    print(rides_per_year_result)\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Your code goes here\n",
    "    rides_per_month_query = \"\"\"\n",
    "                        WITH name1 AS \n",
    "                         (\n",
    "                             SELECT EXTRACT(YEAR FROM trip_start_timestamp) AS year, \n",
    "                             EXTRACT(MONTH FROM trip_start_timestamp) AS month\n",
    "                             FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                         )\n",
    "                         SELECT month, COUNT(1) AS num_trips\n",
    "                         FROM name1\n",
    "                         WHERE year=2017\n",
    "                         GROUP BY month\n",
    "                         ORDER BY month\n",
    "                            \"\"\"\n",
    "\n",
    "    # OR\n",
    "\n",
    "    rides_per_month_query = \"\"\"\n",
    "                        SELECT EXTRACT(MONTH FROM trip_start_timestamp) AS month, \n",
    "                               COUNT(1) AS num_trips\n",
    "                        FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                        WHERE EXTRACT(YEAR FROM trip_start_timestamp) = 2017\n",
    "                        GROUP BY month\n",
    "                        ORDER BY month\n",
    "                        \"\"\"\n",
    "\n",
    "    # Set up the query\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    rides_per_month_query_job = client.query(rides_per_month_query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    rides_per_month_result = rides_per_month_query_job.to_dataframe()  # Your code goes here\n",
    "\n",
    "    # View results\n",
    "    print(rides_per_month_result)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Assuming year-month-day = 2017-01-01 and 2017-07-01\n",
    "    # Write a query that shows, for each hour of the day in the dataset, the corresponding number of trips and average speed.\n",
    "\n",
    "    # Your results should have three columns:\n",
    "\n",
    "        # hour_of_day - sort by this column, which holds the result of extracting the hour from trip_start_timestamp.\n",
    "        # num_trips - the count of the total number of trips in each hour of the day (e.g. how many trips were started between 6AM and 7AM, independent of which day it occurred on).\n",
    "        # avg_mph - the average speed, measured in miles per hour, for trips that started in that hour of the day. Average speed in miles per hour is calculated as 3600 * SUM(trip_miles) / SUM(trip_seconds). (The value 3600 is used to convert from seconds to hours.)\n",
    "\n",
    "    # Restrict your query to data meeting the following criteria:\n",
    "\n",
    "        # a trip_start_timestamp between 2017-01-01 and 2017-07-01\n",
    "        # trip_seconds > 0 and trip_miles > 0\n",
    "\n",
    "    # This is what they asked : but not correct because I use EXTRACT instead of trip_start_timestamp < '2017-07-01'.  Evaluating the Year and month individually should give the same result, as the consolidated format...\n",
    "    speeds_query = \"\"\"\n",
    "               WITH RelevantRides AS\n",
    "               (\n",
    "                   SELECT EXTRACT(HOUR FROM trip_start_timestamp) AS hour_of_day,\n",
    "                   trip_miles, trip_seconds\n",
    "                   FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                   WHERE EXTRACT(HOUR FROM trip_start_timestamp) >= 06 and EXTRACT(HOUR FROM trip_start_timestamp) <= 07 and EXTRACT(YEAR FROM trip_start_timestamp) = 2017 and EXTRACT(MONTH FROM trip_start_timestamp) <= 07 and EXTRACT(MONTH FROM trip_start_timestamp) >= 01 and trip_seconds > 0 and trip_miles > 0\n",
    "               )\n",
    "               SELECT hour_of_day,\n",
    "               COUNT(1) AS num_trips,\n",
    "               3600*SUM(trip_miles)/SUM(trip_seconds) AS avg_mph\n",
    "               FROM RelevantRides\n",
    "               GROUP BY hour_of_day\n",
    "               ORDER BY hour_of_day\n",
    "               \"\"\"\n",
    "    # LESSON : I looked at MINUTE, DAY, and SECOND separately with hour_of_day, by adding it to GROUP BY.  If you group by several things the count values go down for each category.\n",
    "\n",
    "    # *** If you want to aggregate information (or sum/count across certain specifications), do not put it as a specification to group by. \n",
    "               \n",
    "    speeds_query = \"\"\"\n",
    "               WITH RelevantRides AS\n",
    "               (\n",
    "                   SELECT EXTRACT(HOUR FROM trip_start_timestamp) AS hour_of_day,\n",
    "                   trip_miles, trip_seconds\n",
    "                   FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                   WHERE EXTRACT(YEAR FROM trip_start_timestamp) = 2017 and EXTRACT(MONTH FROM trip_start_timestamp) <= 07 and EXTRACT(MONTH FROM trip_start_timestamp) >= 01 and trip_seconds > 0 and trip_miles > 0\n",
    "               )\n",
    "               SELECT hour_of_day,\n",
    "               COUNT(1) AS num_trips,\n",
    "               3600*SUM(trip_miles)/SUM(trip_seconds) AS avg_mph\n",
    "               FROM RelevantRides\n",
    "               GROUP BY hour_of_day\n",
    "               ORDER BY hour_of_day\n",
    "               \"\"\"           \n",
    "               \n",
    "    # OR\n",
    "    \n",
    "    # Given answer : but displays all the hour_of_day, not from 6 to 7 as it instructed\n",
    "    speeds_query = \"\"\"\n",
    "               WITH RelevantRides AS\n",
    "               (\n",
    "                   SELECT EXTRACT(HOUR FROM trip_start_timestamp) AS hour_of_day,\n",
    "                          trip_miles, \n",
    "                          trip_seconds\n",
    "                   FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                   WHERE trip_start_timestamp > '2017-01-01' AND \n",
    "                         trip_start_timestamp < '2017-07-01' AND \n",
    "                         trip_seconds > 0 AND \n",
    "                         trip_miles > 0\n",
    "               )\n",
    "               SELECT hour_of_day,\n",
    "                      COUNT(1) AS num_trips, \n",
    "                      3600 * SUM(trip_miles) / SUM(trip_seconds) AS avg_mph\n",
    "               FROM RelevantRides\n",
    "               GROUP BY hour_of_day\n",
    "               ORDER BY hour_of_day\n",
    "               \"\"\"\n",
    "    # Set up the query (cancel the query if it would use too much of \n",
    "    # your quota)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    speeds_query_job = client.query(speeds_query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    speeds_result = speeds_query_job.to_dataframe()\n",
    "\n",
    "    # View results\n",
    "    print(speeds_result)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # MINUTE, SECOND, MILLISECOND, MICROSECOND\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # INNER JOIN\n",
    "    # ----------------\n",
    "    \n",
    "    # Concatenates rows from two different tables.\n",
    "    \n",
    "    # FROM `project_name.dataset_name.table_left_name` AS table_left_name\n",
    "    # INNER JOIN `project_name.dataset_name.table_right_name` AS table_right_name \n",
    "    # ON table_left_name.matching_column_name = table_right_name.matching_column_name\n",
    "    \n",
    "    # Select the columns that are identical within each table, such that you map the rows of one table to the rows of the other table\n",
    "    \n",
    "    # p.ID = o.Pet_ID means that in table p the column ID is the same as column Pet_ID in table o\n",
    "    \n",
    "    \n",
    "    query_with_CTE = \"\"\" \n",
    "                     SELECT p.Name AS Pet_Name, o.Name AS Owner_Name\n",
    "                     FROM `bigquery-public-data.pet_records.pets` AS p\n",
    "                     INNER JOIN `bigquery-public-data.pet_records.owners` AS o ON p.ID = o.Pet_ID\n",
    "                     \"\"\"\n",
    "    # ON determines which column in each table to use to combine the tables.\n",
    "    \n",
    "    # The joined table only has rows of table one and two that are filled\n",
    "    # If a row is missing a value in one table or both, the joined table will not include the row\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Join the tables QUESTION, ANSWERS in a dataframe\n",
    "    query = \"\"\"\n",
    "        SELECT q.id AS q_id, q.title, q.creation_date AS q_creation_date, q.parent_id, a.id, a.title, a.creation_date AS a_creation_date, a.parent_id\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "        INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a ON q.id = a.parent_id\n",
    "        WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01' and a.creation_date >= '2018-01-01' and a.creation_date < '2018-02-01'\n",
    "        ORDER BY q_id\n",
    "        \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "    df_join = query_job.to_dataframe()\n",
    "    df_join.head()\n",
    "    \n",
    "    # ----------------\n",
    "\n",
    "    # Query to determine the number of files per license, sorted by number of files\n",
    "    query = \"\"\"\n",
    "            SELECT L.license, COUNT(1) AS number_of_files\n",
    "            FROM `bigquery-public-data.github_repos.sample_files` AS sf\n",
    "            INNER JOIN `bigquery-public-data.github_repos.licenses` AS L \n",
    "                ON sf.repo_name = L.repo_name\n",
    "            GROUP BY L.license\n",
    "            ORDER BY number_of_files DESC\n",
    "            \"\"\"\n",
    "\n",
    "    # Set up the query (cancel the query if it would use too much of \n",
    "    # your quota, with the limit set to 10 GB)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    query_job = client.query(query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and convert the results to a pandas DataFrame\n",
    "    file_count_by_license = query_job.to_dataframe()\n",
    "\n",
    "\n",
    "\n",
    "    # posts_questions - column called tags (lists topic of question)\n",
    "    # posts_answers - column called parent_id (ID of question), column owner_user_id (user ID who answered)\n",
    "    # join the two tables - determine the tags for each answer, get the owner_user_id for each tag\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    answers_query = \"\"\"\n",
    "            SELECT pa.id, pa.body, pa.owner_user_id\n",
    "            FROM `bigquery-public-data.stackoverflow.posts_answers` AS pa\n",
    "            INNER JOIN `bigquery-public-data.stackoverflow.posts_questions` AS pq \n",
    "                ON pa.parent_id = pq.id\n",
    "            WHERE pq.tags LIKE '%bigquery%'\n",
    "            \"\"\"\n",
    "        \n",
    "    # Set up the query\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=27*10**10)\n",
    "    answers_query_job = client.query(answers_query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    answers_results = answers_query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "    # Preview results\n",
    "    print(answers_results.head())\n",
    "\n",
    "    # ----------------\n",
    "            \n",
    "    # To find a very precise search pattern\n",
    "            \n",
    "    # WHERE REGEXP_MATCH (repository_name, r'^node-[[:alnum:]]+js$')\n",
    "    \n",
    "    # Here we only want matches where repository_name begins with the word node followed by a hyphen, then contains at least one or more alphanumeric characters, followed by js as the final two characters. \n",
    "    \n",
    "    # possible matches could be node-jinjs, node-ldapjs, node-xml2js\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # List of users who have answered many questions\n",
    "    # Show the owner_user_id column from the posts_answers (call it user_id), and the number of answers each unique user wrote to \"bigquery\"-related questions (call it number_of_answers)\n",
    "    # The user needs to have answered at least one question.\n",
    "    \n",
    "    bigquery_experts_query = \"\"\"\n",
    "                SELECT pa.owner_user_id AS user_id, \n",
    "                COUNT(pa.owner_user_id) AS number_of_answers\n",
    "                FROM `bigquery-public-data.stackoverflow.posts_answers` AS pa\n",
    "                INNER JOIN `bigquery-public-data.stackoverflow.posts_questions` AS pq \n",
    "                    ON pa.parent_id = pq.id\n",
    "                WHERE pq.tags LIKE '%bigquery%'\n",
    "                GROUP BY pa.owner_user_id\n",
    "                HAVING COUNT(pa.owner_user_id) > 0\n",
    "                ORDER BY COUNT(pa.owner_user_id) DESC\n",
    "                            \"\"\"\n",
    "    # Set up the query\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)\n",
    "    bigquery_experts_query_job = client.query(bigquery_experts_query, job_config=safe_config) # Your code goes here\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    bigquery_experts_results = bigquery_experts_query_job.to_dataframe() # Your code goes here\n",
    "\n",
    "    # Preview results\n",
    "    print(bigquery_experts_results.head())\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # This joins the table post_questions to table post_answers, using only intersection values (INNER JOIN)\n",
    "    query = \"\"\"\n",
    "        SELECT q.id AS q_id, q.title, q.creation_date AS q_creation_date, q.parent_id, a.id, a.title, a.creation_date AS a_creation_date, a.parent_id\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "        INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a ON q.id = a.parent_id\n",
    "        WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01' and a.creation_date >= '2018-01-01' and a.creation_date < '2018-02-01'\n",
    "        ORDER BY q_id\n",
    "        \"\"\"\n",
    "\n",
    "    query_job = client.query(query)\n",
    "    df_join = query_job.to_dataframe()\n",
    "    df_join.head()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # How long does it take for questions to receive answers?\n",
    "    # Can change from FULL JOIN, INNER JOIN, LEFT JOIN\n",
    "    \n",
    "    first_query = \"\"\"\n",
    "        SELECT q.id, TIMESTAMP_DIFF(a.creation_date, q.creation_date, SECOND) AS time_to_answer\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "        FULL JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a ON q.id = a.parent_id\n",
    "        WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'\n",
    "        ORDER BY time_to_answer DESC\n",
    "        \"\"\"\n",
    "    first_result = client.query(first_query).result().to_dataframe()\n",
    "    print(\"Percentage of answered questions: %s%%\" % \\\n",
    "          (sum(first_result[\"time_to_answer\"].notnull()) / len(first_result) * 100))\n",
    "    print(\"Number of questions:\", len(first_result))\n",
    "    first_result.head()\n",
    "    \n",
    "    # OR\n",
    "    \n",
    "    # If you put MIN around TIMESTAMP_DIFF you can GROUP BY q_id alone (normally you will get an error because not all the values of q_id can be matched with time_to_answer...I guess it works because it only retuns q_id with the minimal time values), otherwise you have to GROUP BY time_to_answer with q_id\n",
    "    correct_query = \"\"\"\n",
    "        SELECT q.id AS q_id, MIN(TIMESTAMP_DIFF(a.creation_date, q.creation_date, DAY)) AS time_to_answer\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "        LEFT JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a ON q.id = a.parent_id\n",
    "        WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'\n",
    "        GROUP BY q_id\n",
    "        ORDER BY time_to_answer\n",
    "        \"\"\"\n",
    "        \n",
    "     #This query is the obvious alternative, but it gives more results than the above\n",
    "     \n",
    "     correct_query = \"\"\"\n",
    "        SELECT q.id AS q_id, TIMESTAMP_DIFF(a.creation_date, q.creation_date, DAY) AS time_to_answer\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "        LEFT JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a ON q.id = a.parent_id\n",
    "        WHERE q.creation_date >= '2018-01-01' and q.creation_date < '2018-02-01'\n",
    "        GROUP BY q_id, time_to_answer\n",
    "        ORDER BY time_to_answer\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    # You're interested in understanding the initial experiences that users typically have with the Stack Overflow website.  Is it more common for users to first ask questions or provide answers?  After signing up, how long does it take for users to first interact with the website?  To explore this further, you draft the (partial) query in the code cell below.\n",
    "    # The query returns a table with three columns:\n",
    "    # - `owner_user_id` - the user ID\n",
    "    # - `q_creation_date` - the first time the user asked a question \n",
    "    # - `a_creation_date` - the first time the user contributed an answer \n",
    "\n",
    "    # You want to keep track of users who have asked questions, but have yet to provide answers.  And, your table should also include users who have answered questions, but have yet to pose their own questions.\n",
    "    q_and_a_query = \"\"\"\n",
    "                    SELECT q.owner_user_id AS owner_user_id,\n",
    "                        MIN(q.creation_date) AS q_creation_date,\n",
    "                        MIN(a.creation_date) AS a_creation_date\n",
    "                    FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "                        FULL JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "                    ON q.owner_user_id = a.owner_user_id \n",
    "                    WHERE q.creation_date >= '2019-01-01' AND q.creation_date < '2019-02-01' \n",
    "                        AND a.creation_date >= '2019-01-01' AND a.creation_date < '2019-02-01'\n",
    "                    GROUP BY owner_user_id\n",
    "                    \"\"\"\n",
    "    # ----------------\n",
    "    \n",
    "    # To join 3 tables\n",
    "    query = \"\"\"\n",
    "                    SELECT o.Name AS Owner_Name,\n",
    "                        p.Name AS Pet_Name,\n",
    "                        t.Treat AS Fav_Treat\n",
    "                    FROM `bigquery-public-data.pet_records.pets` AS p\n",
    "                    FULL JOIN `bigquery-public-data.pet_records.owners` AS o\n",
    "                    ON p.ID = o.Pet_ID\n",
    "                    LEFT JOIN `bigquery-public-data.pet_records.treats` AS t\n",
    "                    ON p.ID = t.Pet_ID\n",
    "                    \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Looking at users that joined the site in January 2019.\n",
    "    # When did they post their first questions and answers\n",
    "    \n",
    "    three_tables_query = \"\"\"\n",
    "        SELECT u.id AS id,\n",
    "        MIN(q.creation_date) AS q_creation_date,\n",
    "        MIN(a.creation_date) AS a_creation_date\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "        FULL JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "        ON q.owner_user_id = a.owner_user_id\n",
    "        FULL JOIN `bigquery-public-data.stackoverflow.users` AS u\n",
    "        ON q.owner_user_id = u.id\n",
    "        WHERE u.creation_date >= '2019-01-01' AND u.creation_date < '2019-02-01'\n",
    "        GROUP BY id\n",
    "                         \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # How many distinct users posted on January 1, 2019?\n",
    "    \n",
    "    all_users_query = \"\"\"\n",
    "        SELECT q.owner_user_id\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "        WHERE EXTRACT(DATE FROM q.creation_date) = '2019-01-01'\n",
    "        UNION DISTINCT\n",
    "        SELECT a.owner_user_id\n",
    "        FROM `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "        WHERE EXTRACT(DATE FROM a.creation_date) = '2019-01-01'\n",
    "              \"\"\"\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Query to select all stories posted on January 1, 2012, with number of comments\n",
    "    join_query = \"\"\"\n",
    "                 WITH c AS\n",
    "                 (\n",
    "                 SELECT parent, COUNT(*) as num_comments\n",
    "                 FROM `bigquery-public-data.hacker_news.comments` \n",
    "                 GROUP BY parent\n",
    "                 )\n",
    "                 SELECT s.id as story_id, s.by, s.title, c.num_comments\n",
    "                 FROM `bigquery-public-data.hacker_news.stories` AS s\n",
    "                 LEFT JOIN c\n",
    "                 ON s.id = c.parent\n",
    "                 WHERE EXTRACT(DATE FROM s.time_ts) = '2012-01-01'\n",
    "                 ORDER BY c.num_comments DESC\n",
    "                 \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    join_result = client.query(join_query).result().to_dataframe()\n",
    "    join_result.head()\n",
    "    \n",
    "    # ----------------\n",
    "\n",
    "    # Query to select all users who posted stories or comments on January 1, 2014\n",
    "    union_query = \"\"\"\n",
    "                  SELECT c.by\n",
    "                  FROM `bigquery-public-data.hacker_news.comments` AS c\n",
    "                  WHERE EXTRACT(DATE FROM c.time_ts) = '2014-01-01'\n",
    "                  UNION DISTINCT\n",
    "                  SELECT s.by\n",
    "                  FROM `bigquery-public-data.hacker_news.stories` AS s\n",
    "                  WHERE EXTRACT(DATE FROM s.time_ts) = '2014-01-01'\n",
    "                  \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    union_result = client.query(union_query).result().to_dataframe()\n",
    "    union_result.head()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # JOIN\n",
    "    # ----------------\n",
    "    \n",
    "    # INNER JOIN : returns the matching rows from both tables (intersection)\n",
    "    # LEFT JOIN : returns left table and matching rows from right table (left values and unique right values)\n",
    "    # FULL JOIN : returns both tables and NULL entries for missing values (all values)\n",
    "    \n",
    "    # They have the same notation as INNER JOIN:\n",
    "    # FROM `project_name.dataset_name.table_left_name` AS table_left_name\n",
    "    # INNER JOIN `project_name.dataset_name.table_right_name` AS table_right_name \n",
    "    # ON table_left_name.matching_column_name = table_right_name.matching_column_name\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Query to select all stories posted on January 1, 2012, with number of comments\n",
    "    join_query = \"\"\"\n",
    "                 WITH c AS\n",
    "                 (\n",
    "                 SELECT parent, COUNT(*) as num_comments\n",
    "                 FROM `bigquery-public-data.hacker_news.comments` \n",
    "                 GROUP BY parent\n",
    "                 )\n",
    "                 SELECT s.id as story_id, s.by, s.title, c.num_comments\n",
    "                 FROM `bigquery-public-data.hacker_news.stories` AS s\n",
    "                 LEFT JOIN c\n",
    "                 ON s.id = c.parent\n",
    "                 WHERE EXTRACT(DATE FROM s.time_ts) = '2012-01-01'\n",
    "                 ORDER BY c.num_comments DESC\n",
    "                 \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    join_result = client.query(join_query).result().to_dataframe()\n",
    "    join_result.head()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # OVER \n",
    "    # ----------------\n",
    "    # COUNT(*) means any column - so it counts the number of rows in the dataframe\n",
    "    \n",
    "    # ROWS BETWEEN 1 PRECEDING AND CURRENT ROW - the previous row and the current row.\n",
    "    # ROWS BETWEEN 3 PRECEDING AND 1 FOLLOWING - the 3 previous rows, the current row, and the following row.\n",
    "    # ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING - all rows in the partition.\n",
    "\n",
    "    \n",
    "    # Query to count the (cumulative) number of trips per day\n",
    "    num_trips_query = \"\"\"\n",
    "                      WITH trips_by_day AS\n",
    "                      (\n",
    "                      SELECT DATE(start_date) AS trip_date,\n",
    "                          COUNT(*) as num_trips\n",
    "                      FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                      WHERE EXTRACT(YEAR FROM start_date) = 2015\n",
    "                      GROUP BY trip_date\n",
    "                      )\n",
    "                      SELECT *,\n",
    "                          SUM(num_trips) \n",
    "                              OVER (\n",
    "                                   ORDER BY trip_date\n",
    "                                   ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "                                   ) AS cumulative_trips\n",
    "                          FROM trips_by_day\n",
    "                      \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    num_trips_result = client.query(num_trips_query).result().to_dataframe()\n",
    "    num_trips_result.head()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    avg_num_trips_query = \"\"\"\n",
    "                          WITH trips_by_day AS\n",
    "                          (\n",
    "                          SELECT DATE(trip_start_timestamp) AS trip_date,\n",
    "                              COUNT(*) as num_trips\n",
    "                          FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                          WHERE trip_start_timestamp >= '2016-01-01' AND trip_start_timestamp < '2018-01-01'\n",
    "                          GROUP BY trip_date\n",
    "                          ORDER BY trip_date\n",
    "                          )\n",
    "                          SELECT trip_date,\n",
    "                              AVG(num_trips) \n",
    "                              OVER (\n",
    "                                   ORDER BY trip_date\n",
    "                                   ROWS BETWEEN 15 PRECEDING AND 15 FOLLOWING\n",
    "                                   ) AS avg_num_trips\n",
    "                          FROM trips_by_day\n",
    "                          \"\"\"\n",
    "                          \n",
    "    # ----------------\n",
    "        \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Query to track beginning and ending stations on October 25, 2015, for each bike\n",
    "    start_end_query = \"\"\"\n",
    "                      SELECT bike_number,\n",
    "                          TIME(start_date) AS trip_time,\n",
    "                          FIRST_VALUE(start_station_id)\n",
    "                              OVER (\n",
    "                                   PARTITION BY bike_number\n",
    "                                   ORDER BY start_date\n",
    "                                   ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "                                   ) AS first_station_id,\n",
    "                          LAST_VALUE(end_station_id)\n",
    "                              OVER (\n",
    "                                   PARTITION BY bike_number\n",
    "                                   ORDER BY start_date\n",
    "                                   ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING\n",
    "                                   ) AS last_station_id,\n",
    "                          start_station_id,\n",
    "                          end_station_id\n",
    "                      FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                      WHERE DATE(start_date) = '2015-10-25' \n",
    "                      \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    start_end_result = client.query(start_end_query).result().to_dataframe()\n",
    "    start_end_result.head()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # RANK\n",
    "    # ----------------\n",
    "    # The query below returns a DataFrame with three columns from the table: `pickup_community_area`, `trip_start_timestamp`, and `trip_end_timestamp`.  \n",
    "\n",
    "    # Amend the query to return an additional column called `trip_number` which shows the order in which the trips were taken from their respective community areas.  So, the first trip of the day originating from community area 1 should receive a value of 1; the second trip of the day from the same area should receive a value of 2.  Likewise, the first trip of the day from community area 2 should receive a value of 1, and so on.\n",
    "    trip_number_query = \"\"\"\n",
    "        SELECT pickup_community_area,\n",
    "            trip_start_timestamp,\n",
    "            trip_end_timestamp, \n",
    "            RANK() OVER (PARTITION BY pickup_community_area ORDER BY trip_start_timestamp ASC) AS trip_number\n",
    "        FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "        WHERE DATE(trip_start_timestamp) = '2017-05-01'\n",
    "        \"\"\"\n",
    "        \n",
    "    trip_number_result = client.query(trip_number_query).result().to_dataframe()\n",
    "    # ----------------\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    # UNION ALL\n",
    "    # ----------------\n",
    "    \n",
    "    # 'joins'/concatenates columns from two different tables, there may be repeating values\n",
    "    \n",
    "   query = \"\"\"\n",
    "            SELECT Age FROM `project_name.dataset_name.table1`\n",
    "            UNION ALL\n",
    "            SELECT Age FROM `project_name.dataset_name.table2`\n",
    "            \"\"\"\n",
    "    \n",
    "    If you do not want repeating values, use :  UNION DISTINCT\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Query to select all users who posted stories or comments on January 1, 2014\n",
    "    union_query = \"\"\"\n",
    "                  SELECT c.by\n",
    "                  FROM `bigquery-public-data.hacker_news.comments` AS c\n",
    "                  WHERE EXTRACT(DATE FROM c.time_ts) = '2014-01-01'\n",
    "                  UNION DISTINCT\n",
    "                  SELECT s.by\n",
    "                  FROM `bigquery-public-data.hacker_news.stories` AS s\n",
    "                  WHERE EXTRACT(DATE FROM s.time_ts) = '2014-01-01'\n",
    "                  \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    union_result = client.query(union_query).result().to_dataframe()\n",
    "    union_result.head()\n",
    "    \n",
    "    # Number of users who posted stories or comments on January 1, 2014\n",
    "    len(union_result)\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    # Edit the query to include an additional prev_break column that shows the length of the break (in minutes) that the driver had before each trip started (this corresponds to the time between trip_start_timestamp of the current trip and trip_end_timestamp of the previous trip). Partition the calculation by taxi_id, and order the results within each partition by trip_start_timestamp.\n",
    "\n",
    "    # Some sample results are shown below, where all rows correspond to the same driver (or taxi_id). Take the time now to make sure that the values in the prev_break column make sense to you!\n",
    "    break_time_query = \"\"\"\n",
    "                      WITH time_diff AS\n",
    "                      (\n",
    "                      SELECT taxi_id, \n",
    "                      trip_start_timestamp, \n",
    "                      trip_end_timestamp,\n",
    "                      TIMESTAMP_DIFF(trip_end_timestamp, trip_start_timestamp, MINUTE) as row_diff\n",
    "                      FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                      WHERE DATE(trip_start_timestamp) = '2017-05-01'\n",
    "                      )\n",
    "                      SELECT taxi_id, \n",
    "                      trip_start_timestamp, \n",
    "                      trip_end_timestamp,\n",
    "                          SUM(row_diff) \n",
    "                              OVER (\n",
    "                                   ORDER BY trip_start_timestamp\n",
    "                                   ROWS BETWEEN 1 PRECEDING AND CURRENT ROW\n",
    "                                   ) AS prev_break\n",
    "                          FROM time_diff\n",
    "                      \"\"\"\n",
    "    break_time_result = client.query(break_time_query).result().to_dataframe()\n",
    "    break_time_result.head()\n",
    "    \n",
    "    # OR \n",
    "    \n",
    "    # LAG (value_expression[, offset [, default_expression]]) \n",
    "    # LAG has to be used in an OVER\n",
    "    \n",
    "    break_time_query = \"\"\"\n",
    "                   SELECT taxi_id,\n",
    "                       trip_start_timestamp,\n",
    "                       trip_end_timestamp,\n",
    "                       TIMESTAMP_DIFF(\n",
    "                           trip_start_timestamp, \n",
    "                           LAG(trip_end_timestamp, 1) OVER (PARTITION BY taxi_id ORDER BY trip_start_timestamp), \n",
    "                           MINUTE) as prev_break\n",
    "                   FROM `bigquery-public-data.chicago_taxi_trips.taxi_trips`\n",
    "                   WHERE DATE(trip_start_timestamp) = '2017-05-01' \n",
    "                   \"\"\"\n",
    "\n",
    "    break_time_result = client.query(break_time_query).result().to_dataframe()\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Nested data : Nested columns have type STRUCT (or type RECORD). \n",
    "    # Data can be stored in tables with a column field, or in a table with multiple fields per column (ie: column 'Toy' can have a field 'Name' and a field 'Type')\n",
    "    query  = \"\"\"\n",
    "        SELECT Name AS Pet_Name, Toy.Name AS Toy_Name, Toy.Type AS Toy_Type\n",
    "        FROM `bigquery-public-data.pet_records.pets_and_toys_type`\n",
    "        \"\"\"\n",
    "    \n",
    "    \n",
    "    # Repeated data : each column field contains a list (ie: column 'Toy' has [Frisbee, Bone, Rope] in field 1)\n",
    "    \n",
    "    query  = \"\"\"\n",
    "        SELECT Name AS Pet_Name, Toy_Type\n",
    "        FROM `bigquery-public-data.pet_records.pets_and_toys_type`,\n",
    "        UNNEST(Toys) AS Toy_Type\n",
    "        \"\"\"\n",
    "        \n",
    "    # Nested and Repeated data:\n",
    "    query  = \"\"\"\n",
    "        SELECT Name AS Pet_Name, t.Name AS Toy_Type, t.Type AS Toy_Type\n",
    "        FROM `bigquery-public-data.pet_records.pets_and_toys_type`,\n",
    "        UNNEST(Toys) AS t\n",
    "        \"\"\"\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    # Create a \"Client\" object\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    # Construct a reference to the \"google_analytics_sample\" dataset\n",
    "    dataset_ref = client.dataset(\"google_analytics_sample\", project=\"bigquery-public-data\")\n",
    "\n",
    "    # Construct a reference to the \"ga_sessions_20170801\" table\n",
    "    table_ref = dataset_ref.table(\"ga_sessions_20170801\")\n",
    "\n",
    "    # API request - fetch the table\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    # Preview the first five lines of the table\n",
    "    client.list_rows(table, max_results=5).to_dataframe()\n",
    "        \n",
    "        \n",
    "\n",
    "    print(\"SCHEMA field for the 'totals' column:\\n\")\n",
    "    print(table.schema[5])\n",
    "\n",
    "    print(\"\\nSCHEMA field for the 'device' column:\\n\")\n",
    "    print(table.schema[7])\n",
    "    \n",
    "    \n",
    "\n",
    "    # Query to count the number of transactions per browser\n",
    "    query = \"\"\"\n",
    "            SELECT device.browser AS device_browser,\n",
    "                SUM(totals.transactions) as total_transactions\n",
    "            FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`\n",
    "            GROUP BY device_browser\n",
    "            ORDER BY total_transactions DESC\n",
    "            \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    result = client.query(query).result().to_dataframe()\n",
    "    result.head()\n",
    "\n",
    "    # ----------------\n",
    "\n",
    "    # Query to determine most popular landing point on the website\n",
    "    query = \"\"\"\n",
    "            SELECT hits.page.pagePath as path,\n",
    "                COUNT(hits.page.pagePath) as counts\n",
    "            FROM `bigquery-public-data.google_analytics_sample.ga_sessions_20170801`, \n",
    "                UNNEST(hits) as hits\n",
    "            WHERE hits.type=\"PAGE\" and hits.hitNumber=1\n",
    "            GROUP BY path\n",
    "            ORDER BY counts DESC\n",
    "            \"\"\"\n",
    "\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    result = client.query(query).result().to_dataframe()\n",
    "    result.head()\n",
    "\n",
    "    # ----------------\n",
    "    \n",
    "    # Print information on all the columns in the table\n",
    "    sample_commits_table.schema\n",
    "    \n",
    "    # Write a query to find the individuals with the most commits in this table in 2016. Your query should return a table with two columns:\n",
    "\n",
    "    # committer_name - contains the name of each individual with a commit (from 2016) in the table\n",
    "    # num_commits - shows the number of commits the individual has in the table (from 2016)\n",
    "\n",
    "    # Sort the table, so that people with more commits appear first.\n",
    "    max_commits_query = \"\"\"\n",
    "                    WITH name1 AS \n",
    "                     (\n",
    "                         SELECT committer.name AS committer_name,\n",
    "                         EXTRACT(YEAR from committer.date) AS year,\n",
    "                         FROM `bigquery-public-data.github_repos.sample_commits`\n",
    "                     )\n",
    "                     SELECT committer_name, \n",
    "                     COUNT(1) AS num_commits\n",
    "                     FROM name1\n",
    "                     WHERE year = 2016\n",
    "                     GROUP BY committer_name\n",
    "                     ORDER BY num_commits DESC\n",
    "                        \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    # Construct a reference to the \"languages\" table\n",
    "    table_ref = dataset_ref.table(\"languages\")\n",
    "\n",
    "    # API request - fetch the table\n",
    "    languages_table = client.get_table(table_ref)\n",
    "\n",
    "    # Preview the first five lines of the table\n",
    "    client.list_rows(languages_table, max_results=5).to_dataframe()\n",
    "    \n",
    "    # Print information on all the columns in the table\n",
    "    languages_table.schema\n",
    "    \n",
    "    # [SchemaField('repo_name', 'STRING', 'NULLABLE', None, (), None),\n",
    "    # SchemaField('language', 'RECORD', 'REPEATED', None, (SchemaField('name', 'STRING', 'NULLABLE', None, (), None), SchemaField('bytes', 'INTEGER', 'NULLABLE', None, (), None)), None)]\n",
    "    \n",
    "    # How many rows are in the table returned by the query below? 6\n",
    "    query = \"\"\"\n",
    "        SELECT *\n",
    "        FROM `bigquery-public-data.github_repos.languages`,\n",
    "        UNNEST(language)\n",
    "        \"\"\"\n",
    "    # Run the query, and return a pandas DataFrame\n",
    "    result = client.query(query).result().to_dataframe()\n",
    "    result.head()\n",
    "    # ----------------\n",
    "    \n",
    "    # What's the most popular programming language?\n",
    "\n",
    "    # Write a query to leverage the information in the `languages` table to determine which programming # languages appear in the most repositories.  The table returned by your query should have two columns:\n",
    "    # - `language_name` - the name of the programming language\n",
    "    # - `num_repos` - the number of repositories in the `languages` table that use the programming language\n",
    "    pop_lang_query = \"\"\"\n",
    "         SELECT l.name as language_name, COUNT(*) as num_repos\n",
    "         FROM `bigquery-public-data.github_repos.languages`,\n",
    "             UNNEST(language) AS l\n",
    "         GROUP BY language_name\n",
    "         ORDER BY num_repos DESC\n",
    "         \"\"\"\n",
    "         \n",
    "    # ----------------\n",
    "    \n",
    "    # Which languages are used in the repository with the most languages?\n",
    "\n",
    "    # For this question, you'll restrict your attention to the repository with name `'polyrabbit/polyglot'`.\n",
    "\n",
    "    # Write a query that returns a table with one row for each language in this repository.  The table should have two columns:\n",
    "    # - `name` - the name of the programming language\n",
    "    # - `bytes` - the total number of bytes of that programming language\n",
    "\n",
    "    # Sort the table by the `bytes` column so that programming languages that take up more space in the repo appear first.\n",
    "     all_langs_query = \"\"\"\n",
    "                  SELECT l.name, l.bytes\n",
    "                  FROM `bigquery-public-data.github_repos.languages`,\n",
    "                      UNNEST(language) as l\n",
    "                  WHERE repo_name = 'polyrabbit/polyglot'\n",
    "                  ORDER BY l.bytes DESC\n",
    "                  \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "   \n",
    "    # show_amount_of_data_scanned() shows the amount of data the query uses.\n",
    "    # show_time_to_run() prints how long it takes for the query to execute.\n",
    "    \n",
    "    from google.cloud import bigquery\n",
    "    from time import time\n",
    "\n",
    "    client = bigquery.Client()\n",
    "\n",
    "    def show_amount_of_data_scanned(query):\n",
    "        # dry_run lets us see how much data the query uses without running it\n",
    "        dry_run_config = bigquery.QueryJobConfig(dry_run=True)\n",
    "        query_job = client.query(query, job_config=dry_run_config)\n",
    "        print('Data processed: {} GB'.format(round(query_job.total_bytes_processed / 10**9, 3)))\n",
    "        \n",
    "    def show_time_to_run(query):\n",
    "        time_config = bigquery.QueryJobConfig(use_query_cache=False)\n",
    "        start = time()\n",
    "        query_result = client.query(query, job_config=time_config).result()\n",
    "        end = time()\n",
    "        print('Time to run: {} seconds'.format(round(end-start, 3)))\n",
    "    \n",
    "    \n",
    "    # To save processing time \n",
    "    # 1) Only select the columns you need\n",
    "    star_query = \"SELECT * FROM `bigquery-public-data.github_repos.contents`\"\n",
    "    show_amount_of_data_scanned(star_query)\n",
    "\n",
    "    basic_query = \"SELECT size, binary FROM `bigquery-public-data.github_repos.contents`\"\n",
    "    show_amount_of_data_scanned(basic_query)\n",
    "    \n",
    "    # Data processed: 2623.284 GB\n",
    "    # Data processed: 2.466 GB\n",
    "    \n",
    "    # 2)  Read less data\n",
    "    more_data_query = \"\"\"\n",
    "                  SELECT MIN(start_station_name) AS start_station_name,\n",
    "                      MIN(end_station_name) AS end_station_name,\n",
    "                      AVG(duration_sec) AS avg_duration_sec\n",
    "                  FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                  WHERE start_station_id != end_station_id \n",
    "                  GROUP BY start_station_id, end_station_id\n",
    "                  LIMIT 10\n",
    "                  \"\"\"\n",
    "    show_amount_of_data_scanned(more_data_query)\n",
    "\n",
    "    less_data_query = \"\"\"\n",
    "                      SELECT start_station_name,\n",
    "                          end_station_name,\n",
    "                          AVG(duration_sec) AS avg_duration_sec                  \n",
    "                      FROM `bigquery-public-data.san_francisco.bikeshare_trips`\n",
    "                      WHERE start_station_name != end_station_name\n",
    "                      GROUP BY start_station_name, end_station_name\n",
    "                      LIMIT 10\n",
    "                      \"\"\"\n",
    "    show_amount_of_data_scanned(less_data_query)\n",
    "    # Data processed: 0.076 GB\n",
    "    # Data processed: 0.06 GB\n",
    "    \n",
    "    \n",
    "    # 3) Avoid N:N JOINs\n",
    "    big_join_query = \"\"\"\n",
    "                 SELECT repo,\n",
    "                     COUNT(DISTINCT c.committer.name) as num_committers,\n",
    "                     COUNT(DISTINCT f.id) AS num_files\n",
    "                 FROM `bigquery-public-data.github_repos.commits` AS c,\n",
    "                     UNNEST(c.repo_name) AS repo\n",
    "                 INNER JOIN `bigquery-public-data.github_repos.files` AS f\n",
    "                     ON f.repo_name = repo\n",
    "                 WHERE f.repo_name IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                 GROUP BY repo\n",
    "                 ORDER BY repo\n",
    "                     \"\"\"\n",
    "    show_time_to_run(big_join_query)\n",
    "\n",
    "    small_join_query = \"\"\"\n",
    "                       WITH commits AS\n",
    "                       (\n",
    "                       SELECT COUNT(DISTINCT committer.name) AS num_committers, repo\n",
    "                       FROM `bigquery-public-data.github_repos.commits`,\n",
    "                           UNNEST(repo_name) as repo\n",
    "                       WHERE repo IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                       GROUP BY repo\n",
    "                       ),\n",
    "                       files AS \n",
    "                       (\n",
    "                       SELECT COUNT(DISTINCT id) AS num_files, repo_name as repo\n",
    "                       FROM `bigquery-public-data.github_repos.files`\n",
    "                       WHERE repo_name IN ( 'tensorflow/tensorflow', 'facebook/react', 'twbs/bootstrap', 'apple/swift', 'Microsoft/vscode', 'torvalds/linux')\n",
    "                       GROUP BY repo\n",
    "                       )\n",
    "                       SELECT commits.repo, commits.num_committers, files.num_files\n",
    "                       FROM commits \n",
    "                       INNER JOIN files\n",
    "                           ON commits.repo = files.repo\n",
    "                       ORDER BY repo\n",
    "                       \"\"\"\n",
    "\n",
    "    show_time_to_run(small_join_query)\n",
    "    \n",
    "    #  Time to run: 11.926 seconds\n",
    "    # Time to run: 4.293 seconds\n",
    "    # ----------------\n",
    "    \n",
    "    query = \"\"\"\n",
    "        WITH LocationsAndOwners AS \n",
    "        (\n",
    "        SELECT * \n",
    "        FROM CostumeOwners co INNER JOIN CostumeLocations cl\n",
    "           ON co.CostumeID = cl.CostumeID\n",
    "        ),\n",
    "        LastSeen AS\n",
    "        (\n",
    "        SELECT CostumeID, MAX(Timestamp)\n",
    "        FROM LocationsAndOwners\n",
    "        GROUP BY CostumeID\n",
    "        )\n",
    "        SELECT lo.CostumeID, Location \n",
    "        FROM LocationsAndOwners lo INNER JOIN LastSeen ls \n",
    "            ON lo.Timestamp = ls.Timestamp AND lo.CostumeID = ls.CostumeID\n",
    "        WHERE OwnerID = MitzieOwnerID\n",
    "    \"\"\"\n",
    "    \n",
    "        # Is there a way to make this faster or cheaper?\n",
    "    query = \"\"\"\n",
    "        WITH CurrentOwnersCostumes AS\n",
    "        (\n",
    "        SELECT CostumeID \n",
    "        FROM CostumeOwners \n",
    "        WHERE OwnerID = MitzieOwnerID\n",
    "        ),\n",
    "        OwnersCostumesLocations AS\n",
    "        (\n",
    "        SELECT cc.CostumeID, Timestamp, Location \n",
    "        FROM CurrentOwnersCostumes cc INNER JOIN CostumeLocations cl\n",
    "            ON cc.CostumeID = cl.CostumeID\n",
    "        ),\n",
    "        LastSeen AS\n",
    "        (\n",
    "        SELECT CostumeID, MAX(Timestamp)\n",
    "        FROM OwnersCostumesLocations\n",
    "        GROUP BY CostumeID\n",
    "        )\n",
    "        SELECT ocl.CostumeID, Location \n",
    "        FROM OwnersCostumesLocations ocl INNER JOIN LastSeen ls \n",
    "            ON ocl.timestamp = ls.timestamp AND ocl.CostumeID = ls.costumeID\n",
    "        \"\"\"\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    \n",
    "    SELECT USER, SHIPPED FROM example_table\n",
    "    \n",
    "    SELECT USER FROM example_table WHERE SHIPPED='YES'\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    SELECT end_station_name FROM `bigquery-public-data.london_bicycles.cycle_hire`;\n",
    "    \n",
    "    SELECT * FROM `bigquery-public-data.london_bicycles.cycle_hire` WHERE duration>=1200;\n",
    "    \n",
    "    # ----------------\n",
    "    \n",
    "    SELECT start_station_name FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name;\n",
    "    \n",
    "    SELECT start_station_name, COUNT(*) FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name;\n",
    "    \n",
    "    SELECT start_station_name, COUNT(*) AS num_starts FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name;\n",
    "    \n",
    "    # Return a table that contains the number of bikeshare rides that begin in each starting station, organized alphabetically by the starting station.\n",
    "    SELECT start_station_name, COUNT(*) AS num FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name ORDER BY start_station_name;\n",
    "    \n",
    "    # Return a table that contains the number of bikeshare rides that begin in each starting station, organized numerically from lowest to highest.\n",
    "    SELECT start_station_name, COUNT(*) AS num FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name ORDER BY num;\n",
    "    \n",
    "    # Return a table that contains the number of bikeshare rides that begin in each starting station, organized numerically from highest to lowest.\n",
    "    SELECT start_station_name, COUNT(*) AS num FROM `bigquery-public-data.london_bicycles.cycle_hire` GROUP BY start_station_name ORDER BY num DESC;\n",
    "    \n",
    "    # ----------------\n",
    "    '''\n",
    "    \n",
    "    \n",
    "from google.cloud import bigquery\n",
    "\n",
    "def expert_finder_bigquery(topic, client):\n",
    "    Returns a DataFrame with the user IDs who have written Stack Overflow answers on a topic.\n",
    "\n",
    "    Inputs:\n",
    "        topic: A string with the topic of interest\n",
    "        client: A Client object that specifies the connection to the Stack Overflow dataset\n",
    "\n",
    "    Outputs:\n",
    "        results: A DataFrame with columns for user_id and number_of_answers. Follows similar logic to bigquery_experts_results shown above.\n",
    "    my_query = \n",
    "               SELECT a.owner_user_id AS user_id, COUNT(1) AS number_of_answers\n",
    "               FROM `bigquery-public-data.stackoverflow.posts_questions` AS q\n",
    "               INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` AS a\n",
    "                   ON q.id = a.parent_Id\n",
    "               WHERE q.tags like '%{topic}%'\n",
    "               GROUP BY a.owner_user_id\n",
    "               HAVING COUNT(a.owner_user_id) > 0\n",
    "               ORDER BY COUNT(a.owner_user_id) DESC\n",
    "               \"\"\"\n",
    "\n",
    "    # Set up the query (a real service would have good error handling for \n",
    "    # queries that scan too much data)\n",
    "    safe_config = bigquery.QueryJobConfig(maximum_bytes_billed=10**10)      \n",
    "    my_query_job = client.query(my_query, job_config=safe_config)\n",
    "\n",
    "    # API request - run the query, and return a pandas DataFrame\n",
    "    results = my_query_job.to_dataframe()\n",
    "\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1749da0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a031eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa0bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
